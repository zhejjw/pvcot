# -*- coding: utf-8 -*-
"""Chain of Code Experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_i9-LeAZyWoYNt0Oejej-JU0xhHahVFj

# Install packages
"""

#!pip install openai==0.28
#!pip install tiktoken
#!pip install tqdm
#!pip install matplotlib

"""# Import packages"""

import numpy as np
import openai
import tiktoken
from tqdm.auto import trange, tqdm
import time
import os
import json
from tqdm import tqdm
import re
from types import NoneType
import multiprocessing.dummy
from io import StringIO
from contextlib import redirect_stdout
import signal
from contextlib import contextmanager
import matplotlib.pyplot as plt

"""# Git clone CoC prompts for BBH and GSM"""

# !git clone --recursive https://github.com/ChengshuLi/BIG-Bench-Hard.git

"""# Cache (to avoid repetitive API calls)"""

GLOBAL_CACHE = dict()
DIRECT_ANSWER_CACHE = dict()
COT_CACHE = dict()
LLM_TRACE_CACHE = dict()

"""# Set up API key"""

openai.api_key = "<OPENAI API KEY>"

"""# Helper functions"""

def remap_keys(mapping):
  return [{'key':k, 'value': v} for k, v in mapping.items()]

def convert_list_to_tuple(l):
  return tuple(convert_list_to_tuple(x) for x in l) if type(l) is list else l

def unmap_keys(mapping):
  return {tuple(convert_list_to_tuple(it) for it in item["key"]): item["value"] for item in mapping}

def get_options(instruction):
  options = {}
  for line in instruction.split("\n"):
    first_word = line.split(" ")[0]
    rest = " ".join(line.split(" ")[1:])
    if re.match("^\([a-zA-Z]\)$", first_word) is not None:
      options[first_word] = rest
  return options

class TimeoutException(Exception): pass

@contextmanager
def time_limit(seconds):
  def signal_handler(signum, frame):
    raise TimeoutException("Timeout")
  signal.signal(signal.SIGALRM, signal_handler)
  signal.alarm(seconds)
  try:
    yield
  finally:
    signal.alarm(0)

def postprocess_answer(answer, test_task):
  # Fix a few common syntax mistakes that LLM tend to make (across all methods)

  # Fix "Yes/No" to "yes/no", "Valid/Invalid" to "valid/invalid"
  if test_task in ["sports_understanding", "formal_fallacies"]:
    answer = answer.lower()

  # Fix "1" or "(A) to "(A)"
  if test_task in ["movie_recommendation", "tracking_shuffled_objects_five_objects"]:
    if answer in ["1", "2", "3", "4", "5"]:
      answer = ["(A)", "(B)", "(C)", "(D)", "(E)"][int(answer) - 1]
    elif answer.lstrip("(").rstrip(")") in ["1", "2", "3", "4", "5"]:
      answer = ["(A)", "(B)", "(C)", "(D)", "(E)"][int(answer.lstrip("(").rstrip(")")) - 1]

  # Fix "2.0" to "2"
  if test_task == "gsm":
    answer = "".join(ch for ch in answer if ch.isdigit() or ch == ".")
    try:
      answer = str(int(answer))
    except:
      pass

  # Fix "(C) 01/02/2008" to "(C)"
  first_word = answer.split(" ")[0]
  if re.match("^\([a-zA-Z]\)$", first_word) is not None:
    answer = first_word

  # Fix "A" to "(A)"
  if re.match("^[a-zA-Z]$", answer) is not None:
    answer = f"({answer})"

  options = get_options(instruct)
  for key, val in options.items():
    # Fix "09/14/2023" to "(A)"
    if answer == val:
      answer = key
    # Fix "(09/14/2023)" to "(A)"
    if answer.lstrip("(").rstrip(")") == val:
      answer = key
    if test_task in ["tracking_shuffled_objects_five_objects", "tracking_shuffled_objects_seven_objects", "tracking_shuffled_objects_three_objects"]:
      # Fix "red" to "(A)", "(red)" to "(A)"
      if answer in val or answer.lstrip("(").rstrip(")") in val:
        answer = key

  return answer

def evaluate(prompt, instruction, verbose=False):
  if MODEL == "direct":
    return evaluate_direct(prompt, instruction, verbose)
  elif MODEL == "cot":
    return evaluate_cot(prompt, instruction, verbose)
  elif "zero-shot" in MODEL:
    return evaluate_zero_shot(instruction, verbose)
  else:
    return run_llm_to_code_interleave_new(prompt, instruction, verbose)

def evaluate_direct(prompt, instruction, verbose=False):
  assert type(instruction) == list
  query = [prompt + "\n\n" + instruct for instruct in instruction]
  if verbose:
    print("#### Query ####")
    for q in query:
      print("####")
      print(q)
  if MODE == "completion":
    answer = query_llm(query, max_tokens=64, stop=["\n"])
  else:
    answer = query_llm_chat(query, max_tokens=64, stop=["\n"])

  if verbose:
    print("#### Answer ####")
    print(answer)

  assert len(query) == len(answer)
  for instruct, ans in zip(instruction, answer):
    direct_answer_cache_key = (PROTOCOL, ENGINE, instruct)
    DIRECT_ANSWER_CACHE[direct_answer_cache_key] = ans

  return answer

cot_answer_token = "Answer: "
def evaluate_cot(prompt, instruction, verbose=False):
  assert type(instruction) == list
  query = [prompt + "\n\n" + instruct for instruct in instruction]
  if verbose:
    print("#### Query ####")
    for q in query:
      print("####")
      print(q)
  if MODE == "completion":
    answer = copy.deepcopy(query_llm(query, max_tokens=512))
  else:
    answer = copy.deepcopy(query_llm_chat(query, max_tokens=512))

  missing_answer = np.array([cot_answer_token not in ans for ans in answer])
  if missing_answer.any():
    # Original query (this ends with 'A:') + empty space (so that 'A:' becomes 'A: ') + original answer + new line + "Answer:" (no space)
    new_query = [qry + " " + ans + "\n" + cot_answer_token.rstrip(" ") for qry, ans, ma in zip(query, answer, missing_answer) if ma]
    if MODE == "completion":
      new_answer = query_llm(new_query, max_tokens=64, stop=["\n"])
    else:
      new_answer = query_llm_chat(new_query, max_tokens=64, stop=["\n"])

    idx = 0
    for i, ma in enumerate(missing_answer):
      if ma:
        # Replace the new answer with: original answer + new line + "Answer: " + new answer
        answer[i] = answer[i] + "\n" + cot_answer_token + new_answer[idx]
        idx += 1
    assert idx == len(new_answer), "new answer should completely fill out the missing part of answer"

    missing_answer = np.array([cot_answer_token not in ans for ans in answer])
    assert not missing_answer.any(), "still missing answer in CoT"

  parsed_answer = [ans.split(cot_answer_token)[1].strip() for ans in answer]
  if verbose:
    for q, a in zip(query, answer):
      print("#### Query ####")
      print(q)
      print("#### Answer ####")
      print(a)

  assert len(query) == len(parsed_answer)
  for instruct, ans in zip(instruction, parsed_answer):
    cot_cache_key = (PROTOCOL, ENGINE, instruct)
    COT_CACHE[cot_cache_key] = ans

  return parsed_answer

cot_answer_token = "Answer: "
code_start_token = "```python"
code_end_token = "```"
def evaluate_zero_shot(instruction, verbose=False):
  assert type(instruction) == list
  if MODEL == "zero-shot-direct":
    system_prompt = """You will be provided with a task description, a question and an answer format.
You should only respond with the final answer in the format "Answer: <answer>" without any explanation."""
  elif MODEL == "zero-shot-cot":
    system_prompt = """You will be provided with a task description, a question and an answer format.
You should think step by step.
In the end, put the final answer in the format "Answer: <answer>"."""
  elif MODEL == "zero-shot-coc-llm-only":
    system_prompt = """You will be provided with a task description, a question and an answer format.
You should write Python code to help solve the problem, if it's helpful.
Any code you write should be in a code block, ```python <code>``` that ends with ```print("Output:", output)```
You should simulate the output of that Python code at the end of the code block in the format "Output: <output>".
In the end, put the final answer in the format "Answer: <answer>"."""
  elif MODEL == "zero-shot-coc-python-only":
    system_prompt = """You will be provided with a task description, a question and an answer format.
You should write Python code to help solve the problem, if it's helpful.
Any code you write should be in a code block, ```python <code>``` that ends with ```print("Output:", output)```
I'll run the code for you and respond with the print statement in the format "Output: <output>".
If there is no code, I will respond with "No code found". If the code doesn't run, I will respond with "Exception: <exception>".
In the end, put the final answer in the format "Answer: <answer>"."""
  else:
    assert False, f"unknown model: {MODEL}"

  final_answer_prompt = "Answer:"
  if verbose:
    print("#### Query ####")
    for q in instruction:
      print("####")
      print(q)
  assert MODE == "chat", "should only do zero shot with chat models"
  answer = copy.deepcopy(query_llm_chat(prompt=instruction, system_prompt=system_prompt, max_tokens=512))
  if MODEL != "zero-shot-coc-python-only":
    missing_answer = np.array([cot_answer_token not in ans for ans in answer])
  else:
    missing_answer = np.array([True for ans in answer])

  if missing_answer.any():
    new_messages = []
    for qry, ans, ma in zip(instruction, answer, missing_answer):
      if ma:
        if MODEL != "zero-shot-coc-python-only":
          new_messages.append([
              {"role": "system", "content": system_prompt},
              {"role": "user", "content": qry},
              {"role": "assistant", "content": ans},
              {"role": "user", "content": final_answer_prompt},
          ])
        else:
          code_to_run = None
          if code_start_token in ans:
            substr = ans.split(code_start_token)[1]
            if code_end_token in substr:
              code_to_run = substr.split(code_end_token)[0]

          if code_to_run is None:
            final_answer_prompt = "No code found" + "\n" + "Answer:"
          else:
            try:
              f = StringIO()
              with redirect_stdout(f):
                with time_limit(120):
                  exec(code_to_run, {}, {})
              output = f.getvalue().strip()
              final_answer_prompt = output + "\n" + "Answer:"
            except Exception as e:
              final_answer_prompt = "Exception: " + str(e) + "\n" + "Answer:"

          new_messages.append([
              {"role": "system", "content": system_prompt},
              {"role": "user", "content": qry},
              {"role": "assistant", "content": ans},
              {"role": "user", "content": final_answer_prompt},
          ])
          if verbose:
            print("#### Query ####")
            print(qry)
            print("#### FULL RESPONSE ####")
            print(ans)
            print("#### CODE TO RUN ####")
            print(code_to_run)
            print("#### FINAL ANSWER PROMPT ####")
            print(final_answer_prompt)
    new_answer = query_llm_chat(prompt=None, max_tokens=64, messages=new_messages, stop=["\n"])
    idx = 0
    for i, ma in enumerate(missing_answer):
      if ma:
        if verbose:
          print("original answer")
          print(answer[i])
          print("new answer")
          print(new_answer[idx])
        answer[i] = answer[i] + "\n" + (new_answer[idx] if cot_answer_token in new_answer[idx] else cot_answer_token + new_answer[idx])
        idx += 1
        if verbose:
          print("final answer")
          print(answer[i])

    assert idx == len(new_answer), "new answer should completely fill out the missing part of answer"

    missing_answer = np.array([cot_answer_token not in ans for ans in answer])
    assert not missing_answer.any(), "still missing answer in CoT"

  # Find the last occurance of cot_answer_token and take whatever that follows
  parsed_answer = [ans[(ans.rindex(cot_answer_token)) + len(cot_answer_token):].strip() for ans in answer]
  if verbose:
    for q, a, pa in zip(instruction, answer, parsed_answer):
      print("#### Query ####")
      print(q)
      print("#### Answer ####")
      print(a)
      print("#### Parsed Answer ####")
      print(pa)
  return parsed_answer

from matplotlib.rcsetup import validate_markevery
from collections import Counter
code_start_token = "# CODE START"
code_end_token = "# CODE END"
trace_start_token = '# TRACE START'
trace_end_token = '# TRACE END'
llm_answer_token = "Answer: "
BUFFER = 100

import sys
import ast
import copy

# Store the lineno of the next error that should be wrapped around by try/except
# Example:
# lineno 4 a = undefined_fn()
# error_lineno will be 4
# Afterwards, this line will be modified to
# lineno 4 try:
# lineno 5   a = undefined_fn()
# lineno 6 except:
# lineno 7   pass
error_lineno = None

# Dict[int, str]: lineno -> the code that the LLM should simulate the execution of
# Note that the lineno will be the line with "pass"
# Example:
# lineno 4 try:
# lineno 5   a = undefined_fn()
# lineno 6 except:
# lineno 7   pass
# errors will be {7: "a = undefined_fn()"}
errors = {}

# Simply a list of lines from the code
# lines = code_to_run.split("\n")
lines = None
trace_lines = None
last_state = None

prompt_plus_code = None
current_instruction = None
lines_run_history = None
def get_delta_state(state, last_state):
  delta_state = {}
  for key, val in state.items():
    if key not in last_state or val != last_state[key]:
      delta_state[key] = val
  return delta_state

def get_state(frame):
  state = {}
  for key, item in frame.f_locals.items():
    if isinstance(item, (bool, str, int, float, tuple, list, set, dict, NoneType)):
      state[key] = item
  return state

def show_trace(frame, event, arg):
  # Declare these global variable first
  global errors
  global error_lineno
  global lines
  global trace_lines
  global last_state
  global prompt_plus_code
  global current_instruction
  global lines_run_history

  # The LLM-generated code will be wrapped around in the get_answer function call.
  # If we don't filter by "get_answer", we got a bunch of random exception from colab
  if frame.f_code.co_name != "get_answer":
    return

  lineno = frame.f_lineno - 1
  # Running a certain line
  if event == "line":
    skipped = False
    current_line = lines[lineno]
    lines_run_history[current_line] += 1
    if lines_run_history[current_line] == 100:
      raise ValueError("infinite loop detected")
    if current_line.strip() in ["try:", "except:", "pass"]:
      skipped = True
      pass
    elif current_line.strip() == "return answer":
      assert lineno == len(lines) - 2, "return answer is at the wrong line" # Second to last line
      state = get_state(frame)
      assert last_state is not None
      delta_state = get_delta_state(state, last_state)
      trace_lines.append(f"delta state: {delta_state}")
      # Append the final state
      print("Add final state before returning")
      trace_lines.append(f"final state: {state}")
      print(f"final state: {state}")
      print("#### Full Trace ####")
      print("=" * 50)
      print("\n".join(trace_lines))
      print("=" * 50)
    elif lineno not in errors:
      # We previous indent 2 spaces
      assert current_line[:2] == "  ", f"Python: actual line to run doesn't have two leading spaces: {current_line} {lines}"
      # Now we revert back
      current_line = current_line[2:]

      state = get_state(frame)
      delta_state = None
      if last_state is None:
        delta_state = None
      else:
        delta_state = get_delta_state(state, last_state)
      last_state = copy.deepcopy(state)

      if delta_state is None:
        trace_lines.append(f"state: {{'question': '{current_instruction}'}}")
      else:
        trace_lines.append(f"delta state: {delta_state}")
      trace_lines.append(f"line: {current_line}")
      trace_lines.append("explanation: Python execution.")
    else:
      # We previous indent 4 spaces
      assert current_line[:4] == "    ", f"LLM: actual line to run doesn't have four leading spaces: {current_line} {lines}"
      # Now we revert back
      current_line = current_line[4:]
      # When LLM excutes, remove any trailing space at the beginning

      state = get_state(frame)
      delta_state = None
      if last_state is None:
        delta_state = None
      else:
        delta_state = get_delta_state(state, last_state)
      last_state = copy.deepcopy(state)

      if delta_state is None:
        trace_lines.append(f"state: {{'question': {current_instruction}}}")
      else:
        trace_lines.append(f"delta state: {delta_state}")
      trace_lines.append(f"line: {current_line}")

      prompt = prompt_plus_code + "\n".join(trace_lines) + "\n" + "explanation:"

      print("#### LLM prompt length ####")
      encoder = tiktoken.encoding_for_model(ENGINE)
      token_length = len(encoder.encode(prompt))
      print(token_length)
      print("#### LLM prompt ####")
      print(prompt)
      max_tokens = 512
      if MODE == "completion":
        llm_result = query_llm([prompt], max_tokens=max_tokens, stop=["\nline:"])[0]
      else:
        llm_result = query_llm_chat([prompt], max_tokens=max_tokens, stop=["\nline:"])[0]
      print("#### LLM result ####")
      print(llm_result)

      explanation = llm_result.split("delta state: ")[0].rstrip("\n")

      trace_lines.append(f"explanation: {explanation}")
      program_state_str = llm_result.split("delta state: ")[1].rstrip("\n")

      print("#### Program State String")
      print(program_state_str)
      try:
        new_program_state = ast.literal_eval(program_state_str)
        assert isinstance(new_program_state, dict), "new program state is not a valid dict"
        # Actually update the local variables with the new program state
        frame.f_locals.update(new_program_state)
      except Exception as e:
        print("#### Fail to update program state ####")
        print(e)
        raise e

  elif event == "exception":
    print(event, lineno, lines[lineno], arg)
    # Only capture the lowest level exception AND if this exception hasn't been "fixed" before, i.e. this line hasn't be sandwiched by try/except yet.
    if error_lineno is None and lineno not in errors:
      error_lineno = lineno
      print("next exception to fix: error_lineno", error_lineno)

  return show_trace

sys.settrace(show_trace)

def breakdown_prompt(prompt, start_token, end_token, remove_from_prompt=False):
  result = []
  start = 0
  assert prompt.count(start_token) == 3, prompt.count(start_token)
  assert prompt.count(end_token) == 3, prompt.count(end_token)
  for _ in range(3):
    assert start_token in prompt and end_token in prompt, "no substring in prompt"
    start_idx = prompt.index(start_token, start)
    end_idx = prompt.index(end_token, start) + len(end_token)
    result.append(prompt[start_idx:end_idx])
    start = end_idx
    if remove_from_prompt:
      prompt = prompt[:start_idx] + prompt[end_idx+1:]
      start = start_idx
  if remove_from_prompt:
    return result, prompt
  else:
    return result

def run_llm_to_code_interleave_new(prompt, instruction, verbose=False):
  # Declare these global variables
  global errors
  global error_lineno
  global lines
  global trace_lines
  global last_state
  global prompt_plus_code
  global current_instruction
  global lines_run_history

  assert type(instruction) == list

  # Each element is "# CODE START\n...\n# CODE_END"
  codes = breakdown_prompt(prompt, code_start_token, code_end_token)
  # Each element is "Answer: ...\n\n"
  fallback_answers = breakdown_prompt(prompt + "\n\n", llm_answer_token, "\n\n")
  ignore_trace = test_task in ["dyck_languages", "geometric_shapes"] and PROTOCOL == "single_task"
  if ignore_trace:
    trace_only = ""
    code_trace_answer = "".join([c + "\n" + f for c, f in zip(codes, fallback_answers)])
  else:
    # Break prompt into 1) everything except trace, and 2) trace-only
    # Each element is "# TRACE START\n...\n# TRACE END"
    traces, prompt = breakdown_prompt(prompt, trace_start_token, trace_end_token, remove_from_prompt=True)
    assert trace_start_token not in prompt and trace_end_token not in prompt, "trace should be teased off now"
    trace_only = "\n\n".join(traces)
    code_trace_answer = "".join([c + "\n" + t + "\n" + f for c, t, f in zip(codes, traces, fallback_answers)])

  query = [prompt + "\n\n" + instruct + "\n" + code_start_token + "\n" for instruct in instruction]
  if verbose:
    print("#### Query ####")
    for q in query:
      print(q)
    print("#### Query End ####")
  if MODE == "completion":
    response_text_list = query_llm(query, max_tokens=1024)
  else:
    assert len(query) == 1
    response_text_list = query_llm_chat(query, max_tokens=1024)

  answers = []
  response_fallback_answers = [response_text.split(llm_answer_token)[1].strip()
    if llm_answer_token in response_text else ""
    for response_text in response_text_list]
  # if LLM execution only, use the fallback answers.
  if MODEL == "coc-llm-only":
    answers = response_fallback_answers
  # if LLM execution only with trace
  if MODEL == "coc-llm-only-trace":
    answers = [None] * len(response_text_list)
    indices_to_fill = []
    llm_code_exec_prompts = []
    for i, response_text in enumerate(response_text_list):
      current_instruction = instruction[i]
      if verbose:
        print("#### Full Response Text ####")
        print(response_text)

      if code_end_token not in response_text:
        print("#### Use direct prompting answer when wrong format ####")
        direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
        answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
        answers[i] = answer
        continue

      code_to_run = response_text.split(code_end_token)[0].strip()
      if verbose:
        print("#### Code ####")
        print(code_to_run)

      if "input(" in code_to_run or "sum_ += j**(2004 - fibonacci(j))" in code_to_run or "b_answer = int(ratio * a_answer)" in code_to_run or "sum += j**(2004 - fibonacci(j))" in code_to_run:
        print("#### Use direct prompting answer when code asks for user input ####")
        direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
        answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
        answers[i] = answer
        continue

      llm_code_exec_prompt = code_trace_answer + code_start_token + "\n" + code_to_run + "\n" + code_end_token + "\n" + trace_start_token + "\n"
      llm_code_exec_prompts.append(llm_code_exec_prompt)
      indices_to_fill.append(i)

    if len(llm_code_exec_prompts) > 0:
      if MODE == "completion":
        print(len(llm_code_exec_prompts))
        llm_responses = query_llm(llm_code_exec_prompts, max_tokens=1024)
      else:
        llm_responses = query_llm_chat(llm_code_exec_prompts, max_tokens=1024)

      if verbose:
        print("#### LLM Full Exec ####")
        print(llm_responses)
      assert len(indices_to_fill) == len(llm_responses)
      for idx, llm_response in zip(indices_to_fill, llm_responses):
        if llm_answer_token in llm_response:
          answer = llm_response.split(llm_answer_token)[1].strip()
        else:
          print("#### Use Fallback when LLM fails to execute and output the final answer ####")
          answer = response_fallback_answers[idx]
        answers[idx] = answer

    for instruct, ans in zip(instruction, answers):
      llm_trace_cache_key = (PROTOCOL, ENGINE, instruct)
      LLM_TRACE_CACHE[llm_trace_cache_key] = ans

  for i, response_text in enumerate(response_text_list):
    # Already handled above, skip
    if MODEL in ["coc-llm-only", "coc-llm-only-trace"]:
      break
    assert len(answers) == i
    current_instruction = instruction[i]

    if verbose:
      print("#### Full Response Text ####")
      print(response_text)

    if code_end_token not in response_text:
      print("#### Use direct prompting answer when wrong format ####")
      direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
      answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
      answers.append(answer)
      continue

    code_to_run = response_text.split(code_end_token)[0].strip()
    if verbose:
      print("#### Code ####")
      print(code_to_run)

    if "input(" in code_to_run or "sum_ += j**(2004 - fibonacci(j))" in code_to_run or "b_answer = int(ratio * a_answer)" in code_to_run or "sum += j**(2004 - fibonacci(j))" in code_to_run:
      print("#### Use direct prompting answer when code asks for user input ####")
      direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
      answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
      answers.append(answer)
      continue

    if llm_answer_token in response_text:
      fallback_answer = response_text.split(llm_answer_token)[1].strip()
      if verbose:
        print("#### Fallback Answer ####")
        print(fallback_answer)
    else:
      fallback_answer = ""

    # Reset their values
    errors = {}
    error_lineno = None
    lines = None
    trace_lines = []
    last_state = None
    lines_run_history = Counter()
    print("#### Original Code ####")
    print(code_to_run)
    prompt_plus_code = trace_only + "\n\n" + trace_start_token + "\n"
    answer = None
    max_trials = 20
    # Wrap the code inside the get_answer function call
    code_to_run_temp = code_to_run.split("\n")
    code_to_run = "\n".join(["  " + l for l in code_to_run_temp])
    code_to_run = f"""def get_answer():
{code_to_run}
  return answer
answer = get_answer()"""
    lines = code_to_run.split("\n")
    local_vars = locals()

    for num_trial in range(max_trials):
      if sys.gettrace() is None: sys.settrace(show_trace)
      assert sys.gettrace() is not None, "get trace is None"
      try:
        # answer will be populated by exec function.
        with time_limit(30):
          exec(code_to_run, globals(), local_vars)
        answer = local_vars["answer"]
        if answer is None:
          if MODEL == "coc-python-only":
            print("#### Use empty string when answer is None ####")
            answer = ""
          elif MODEL == "coc-try-python-except-direct":
            print("#### Use direct prompting answer when answer is None ####")
            direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
            answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
          elif MODEL == "coc-try-python-except-cot":
            print("#### Use CoT answer when answer is None ####")
            cot_cache_key = (PROTOCOL, ENGINE, current_instruction)
            answer = COT_CACHE[cot_cache_key]
          elif MODEL in ["coc-try-python-except-llm", "coc"]:
            print("#### Use Fallback when answer is None ####")
            answer = fallback_answer
          elif MODEL == "coc-try-python-except-llm-trace":
            print("#### Use LLM trace answer when answer is None ####")
            llm_trace_cache_key = (PROTOCOL, ENGINE, current_instruction)
            answer = LLM_TRACE_CACHE[llm_trace_cache_key]

        assert answer is not None
        answers.append(answer)
        break
      except Exception as e:  # exec could fails because of 1) syntax error, 2) runtime error, 3) timeout error
        # If code fails to execute, returns a trivial answer
        if MODEL == "coc-python-only":
          print("#### Use empty string when python fails to run ####")
          answer = ""
          answers.append(answer)
          break
        # If code fails to execute, return the direct prompting answer
        elif MODEL == "coc-try-python-except-direct":
          print("#### Use direct prompting answer when code fails to run ####")
          direct_answer_cache_key = (PROTOCOL, ENGINE, current_instruction)
          answer = DIRECT_ANSWER_CACHE[direct_answer_cache_key]
          answers.append(answer)
          break
        elif MODEL == "coc-try-python-except-cot":
          print("#### Use CoT answer when code fails to run ####")
          cot_cache_key = (PROTOCOL, ENGINE, current_instruction)
          answer = COT_CACHE[cot_cache_key]
          answers.append(answer)
          break
        # Fall back to LLM output
        elif MODEL == "coc-try-python-except-llm":
          print("#### Use Fallback when code fails to execute ####")
          answer = fallback_answer
          answers.append(answer)
          break
        elif MODEL == "coc-try-python-except-llm-trace":
          print("#### Use LLM trace answer when code fails to execute ####")
          llm_trace_cache_key = (PROTOCOL, ENGINE, current_instruction)
          answer = LLM_TRACE_CACHE[llm_trace_cache_key]
          answers.append(answer)
          break

        print("Exception", e)
        print(type(e))
        print("error_lineno", error_lineno)

        # This normally wouldn't happen. This would happen if exec raises a static error (e.g. SyntaxError or IndentationError). Static errors won't be cached by the settrace debugger.
        if error_lineno is None or isinstance(e, TimeoutException):
          print("#### Use Fallback when error_lineno is None ####")
          answer = fallback_answer
          answers.append(answer)
          break

        # Update errors
        line = lines[error_lineno]
        errors[error_lineno + 1] = line

        # Update lines and code_to_run
        num_indent = len(line) - len(line.lstrip())
        lines[error_lineno] = " " * 2 + lines[error_lineno]
        lines.insert(error_lineno, " " * num_indent + "try:")
        lines.insert(error_lineno + 2, " " * num_indent + "except:")
        lines.insert(error_lineno + 3, " " * (num_indent + 2) + "pass")
        code_to_run = "\n".join(lines)

        if verbose:
          print("#### New Code ####")
          print(code_to_run)

        # Reset error_lineno and trace_lines
        error_lineno = None
        trace_lines = []
        last_state = None
        lines_run_history = Counter()

    print("#### Original Query ####")
    print(instruction[i])
    print("#### Final Answer ####")
    print(answer)

  return answers

def query_llm(prompt, max_tokens, stop=None, temperature=0):
  assert type(prompt) == list
  assert max_tokens < MAX_TOKEN_LENGTH, f"requested max tokens ({max_tokens}) exceed model max tokens ({MAX_TOKEN_LENGTH})"
  # If the requested max_tokens exceed the limit, truncate the prompt (effectively reduce the number of examples in the prompt, e.g. from 3 examples to 2.5 examples)
  for i in range(len(prompt)):
    p = prompt[i]
    remaining_max_tokens = MAX_TOKEN_LENGTH - max_tokens
    current_tokens = len(ENCODER.encode(p))
    if current_tokens > remaining_max_tokens:
      current_chars = len(p)
      required_chars = int(float(current_chars) / current_tokens * remaining_max_tokens)
      assert required_chars > 0
      while len(ENCODER.encode(p[-required_chars:])) > remaining_max_tokens:
        required_chars -= 10
      prompt[i] = p[-required_chars:]

  global_cache_key = (tuple(prompt), ENGINE, max_tokens, temperature, stop if stop is None else tuple(stop))
  if global_cache_key not in GLOBAL_CACHE:
    response = openai.Completion.create(prompt=prompt, model=ENGINE, max_tokens=max_tokens, temperature=temperature, stop=stop)
    GLOBAL_CACHE[global_cache_key] = [choice["text"].strip() for choice in response.choices]
  response_text = GLOBAL_CACHE[global_cache_key]
  return response_text

def query_llm_chat(prompt, max_tokens, system_prompt=None, messages=None, stop=None, temperature=0):
  overwrite_messages = False
  if prompt is not None:
    assert messages is None
    assert type(prompt) == list
    messages = [None] * len(prompt)
  else:
    assert messages is not None
    assert type(messages) == list
    prompt = [None] * len(messages)
    overwrite_messages = True

  assert max_tokens < MAX_TOKEN_LENGTH, f"requested max tokens ({max_tokens}) exceed model max tokens ({MAX_TOKEN_LENGTH})"

  # This is using the Chat model as if it's a Completion model
  # We assume we only need truncation in this scenario.
  # When using the Chat model as a Chat model, we will do zero-shot (with high-level instruction),
  # which will be much shorter than 3 examples if we do few-shot. Hence no truncation is needed.
  if prompt is not None and system_prompt is None and messages is None:
    # If the requested max_tokens exceed the limit, truncate the prompt (effectively reduce the number of examples in the prompt, e.g. from 3 examples to 2.5 examples)
    for i in range(len(prompt)):
      p = prompt[i]
      remaining_max_tokens = MAX_TOKEN_LENGTH - max_tokens
      current_tokens = len(ENCODER.encode(p))
      if current_tokens > remaining_max_tokens:
        current_chars = len(p)
        required_chars = int(float(current_chars) / current_tokens * remaining_max_tokens)
        assert required_chars > 0
        while len(ENCODER.encode(p[-required_chars:])) > remaining_max_tokens:
          required_chars -= 10
        prompt[i] = p[-required_chars:]

  global_cache_key = (tuple(prompt), system_prompt, tuple(tuple((mm["role"], mm["content"]) for mm in m) for m in messages) if overwrite_messages else tuple(messages), ENGINE, max_tokens, temperature, stop if stop is None else tuple(stop))
  if global_cache_key not in GLOBAL_CACHE:
    pool = multiprocessing.dummy.Pool(len(prompt))
    while True:
      results = async_get_promises([pool.apply_async(query_llm_chat_single, (p, max_tokens, system_prompt, m, stop, temperature)) for p, m in zip(prompt, messages)])
      if None not in results:
        break
      # Sleep for the OpenAI API quota limit
      print("Some subprocesses don't finish in time, possibly due to OpenAI rate limit.")
      print("Sleep for 30 seconds")
      time.sleep(30)
    GLOBAL_CACHE[global_cache_key] = results
  response_text = GLOBAL_CACHE[global_cache_key]

  return response_text

def query_llm_chat_single(prompt, max_tokens, system_prompt=None, messages=None, stop=None, temperature=0):
  if messages is None:
    assert type(prompt) == str
    messages = []
    if system_prompt is not None:
      messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

  response = openai.ChatCompletion.create(
    model=ENGINE,
    messages=messages,
    max_tokens=max_tokens, # default to be inf
    stop=stop,
    temperature=temperature,
  )
  response_text = response["choices"][0]["message"]["content"].strip()
  return response_text

def async_get_promises(promises):
  n_promises = len(promises)
  responses_dict = {}

  start_time = time.time()
  while len(responses_dict) < n_promises:
    # Everything should return within 30 seconds
    if time.time() - start_time > 30.0:
      break
    for idx, promise in enumerate(promises):
      if idx not in responses_dict and promise.ready():
        responses_dict[idx] = promise.get()
    time.sleep(0.1)

  # If timeout, fill in the remaining missing entries with None
  for idx, promise in enumerate(promises):
    if idx not in responses_dict:
      responses_dict[idx] = None

  return [responses_dict[idx] for idx in range(n_promises)]

"""# Set up experiments"""

max_token_lengths = {
  "text-ada-001": 2048,
  "text-babbage-001": 2048,
  "text-curie-001": 2048,
  "text-davinci-002": 4096,
  "text-davinci-003": 4096,
  "gpt-3.5-turbo-instruct": 4096,
  "gpt-3.5-turbo": 4096,
  "gpt-4": 8192,
}
model_types = {
  "text-ada-001": "completion",
  "text-babbage-001": "completion",
  "text-curie-001": "completion",
  "text-davinci-002": "completion",
  "text-davinci-003": "completion",
  "gpt-3.5-turbo-instruct": "completion",
  "gpt-3.5-turbo": "chat",
  "gpt-4": "chat",
}
MODE = "completion" # @param ["completion", "chat"]
assert MODE in ["completion", "chat"]
ENGINE = "gpt-3.5-turbo-instruct" # @param ["text-davinci-002", "text-davinci-003", "gpt-3.5-turbo", "gpt-4", "text-ada-001", "text-babbage-001", "text-curie-001", "gpt-3.5-turbo-instruct"]
assert ENGINE in ["text-davinci-002", "text-davinci-003", "gpt-3.5-turbo", "gpt-4", "text-ada-001", "text-babbage-001", "text-curie-001", "gpt-3.5-turbo-instruct"]
assert model_types[ENGINE] == MODE
MAX_TOKEN_LENGTH = max_token_lengths[ENGINE]
ENCODER = tiktoken.encoding_for_model(ENGINE)
MODEL = "coc" # @param ["direct", "cot", "coc-python-only", "coc-llm-only", "coc-llm-only-trace", "coc-try-python-except-llm", "coc-try-python-except-llm-trace", "coc-try-python-except-direct", "coc-try-python-except-cot", "coc", "zero-shot-direct", "zero-shot-cot", "zero-shot-coc-llm-only", "zero-shot-coc-python-only"]
assert MODEL in [
  "direct",
  "cot",
  "coc-python-only",
  "coc-llm-only",
  "coc-llm-only-trace",
  "coc-try-python-except-llm",
  "coc-try-python-except-llm-trace",
  "coc-try-python-except-direct",
  "coc-try-python-except-cot",
  "coc",
  "zero-shot-direct",
  "zero-shot-cot",
  "zero-shot-coc-llm-only",
  "zero-shot-coc-python-only",
], MODEL
PROMPT_MODEL = "coc" if MODEL in [
  "coc-python-only",
  "coc-llm-only",
  "coc-llm-only-trace",
  "coc-try-python-except-llm",
  "coc-try-python-except-llm-trace",
  "coc-try-python-except-direct",
  "coc-try-python-except-cot",
  "coc",
  # prompt doesn't matter because it's zero shot
  "zero-shot-direct",
  "zero-shot-cot",
  "zero-shot-coc-llm-only",
  "zero-shot-coc-python-only",
] else MODEL
PROTOCOL = "single_task" # @param ["single_task", "across_task"]
assert PROTOCOL in ["single_task", "across_task"]
BATCH_SIZE = 20

bbh = "BIG-Bench-Hard"
bbh_problems = f"{bbh}/bbh"
bbh_prompts = f"{bbh}/{PROMPT_MODEL}-prompts"
bbh_outputs = f"{bbh}/outputs/{PROTOCOL}/{ENGINE}/{MODEL}"
if not os.path.isdir(bbh_outputs):
  os.makedirs(bbh_outputs, exist_ok=True)
print(bbh_outputs)

# with open(f"{bbh}/outputs/global_cache.json", "r") as f:
#   GLOBAL_CACHE = json.load(f)
# GLOBAL_CACHE = unmap_keys(GLOBAL_CACHE)

# with open(f"{bbh}/outputs/direct_answer_cache.json", "r") as f:
#   DIRECT_ANSWER_CACHE = json.load(f)
# DIRECT_ANSWER_CACHE = unmap_keys(DIRECT_ANSWER_CACHE)

# with open(f"{bbh}/outputs/cot_cache.json", "r") as f:
#   COT_CACHE = json.load(f)
# COT_CACHE = unmap_keys(COT_CACHE)

# with open(f"{bbh}/outputs/llm_trace_cache.json", "r") as f:
#   LLM_TRACE_CACHE = json.load(f)
# LLM_TRACE_CACHE = unmap_keys(LLM_TRACE_CACHE)

def build_prompt(prompt_setup):
  gathered_examples = []
  for task, example_indices in prompt_setup.items():
    with open(os.path.join(bbh_prompts, f"{task}.txt")) as f:
      raw_text = f.read()
      example_start_idx = raw_text.index("Q: ")
      raw_text_examples = raw_text[example_start_idx:]
      examples = raw_text_examples.strip("\n").split("\n\n")
      assert len(examples) == num_examples_per_task, (task, len(examples))
      for idx in example_indices:
        original_example = examples[idx]
        answer_idx = original_example.index("A:")
        augmented_example = ""
        augmented_example += "Task description: " + task_description[task] + "\n"
        augmented_example += original_example[:answer_idx]
        augmented_example += original_example[answer_idx:]
        gathered_examples.append(augmented_example)

  prompt = "\n\n".join(gathered_examples)
  return prompt

task_description = {
  'boolean_expressions': "Evaluate the result of a random Boolean expression.",
  'causal_judgement': "Answer questions about causal attribution.",
  'date_understanding': "Infer the date from context.",
  'disambiguation_qa': "Clarify the meaning of sentences with ambiguous pronouns.",
  'dyck_languages': "Correctly close a Dyck-n word.",
  'formal_fallacies': "Distinguish deductively valid arguments from formal fallacies.",
  'geometric_shapes': "Name geometric shapes from their SVG paths.",
  'gsm': "Solve a grade school math problem.",  # GSM
  'hyperbaton': "Order adjectives correctly in English sentences.",
  'logical_deduction_five_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  'logical_deduction_seven_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  'logical_deduction_three_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  'movie_recommendation': "Recommend movies similar to the given list of movies.",
  'multistep_arithmetic_two': "Solve multi-step arithmetic problems.",
  'navigate': "Given a series of navigation instructions, determine whether one would end up back at the starting point.",
  'object_counting': "Questions that involve enumerating objects and asking the model to count them.",
  'penguins_in_a_table': "Answer questions about a table of penguins and their attributes.",
  'reasoning_about_colored_objects': "Answer extremely simple questions about the colors of objects on a surface.",
  'ruin_names': "Select the humorous edit that 'ruins' the input movie or musical artist name.",
  'salient_translation_error_detection': "Detect the type of error in an English translation of a German source sentence.",
  'snarks': "Determine which of two sentences is sarcastic.",
  'sports_understanding': "Determine whether an artificially constructed sentence relating to sports is plausible or not.",
  'temporal_sequences': "Answer questions about which times certain events could have occurred.",
  'tracking_shuffled_objects_five_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  'tracking_shuffled_objects_seven_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  'tracking_shuffled_objects_three_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  'web_of_lies': "Evaluate a random boolean function expressed as a word problem.",
  'word_sorting': "Sort a list of words.",
}
task_answer_format = {
  'boolean_expressions': "The answer should be either True or False, with the first letter capitalized.",
  'causal_judgement': "The answer should be either Yes or No, with the first letter capitalized.",
  'date_understanding': "The answer should be one of the options above. It should be either (A) or (B) or (C) or (D) or (E), not the actual date itself.",
  'disambiguation_qa': "The answer should be one of the options above. It should be either (A) or (B) depending on which noun the pronoun refers to, or (C) if the pronoun can refer to both.",
  'dyck_languages': "The answer should be a list of brackets/parentheses, separated by spaces, e.g. } ] >.",
  'formal_fallacies': "The answer should be either valid or invalid, all lowercase.",
  'geometric_shapes': "The answer should be one of the options above, e.g. (A).",
  'gsm': "The answer should be an integer number.",  # GSM
  'hyperbaton': "The answer should be one of the options above. It should be either (A) or (B).",
  'logical_deduction_five_objects': "The answer should be one of the options above. It should be either (A) or (B) or (C) or (D) or (E).",
  'logical_deduction_seven_objects': "The answer should be one of the options above. It should be either (A) or (B) or (C) or (D) or (E) or (F) or (G).",
  'logical_deduction_three_objects': "The answer should be one of the options above. It should be either (A) or (B) or (C).",
  'movie_recommendation': "The answer should be one of the options above. It should be either (A) or (B) or (C) or (D), not the actual movie itself.",
  'multistep_arithmetic_two': "The answer should be an integer number.",
  'navigate': "The answer should be either Yes or No, with the first letter capitalized.",
  'object_counting': "The answer should be an integer number.",
  'penguins_in_a_table': "The answer should be one of the options above, e.g. (A).",
  'reasoning_about_colored_objects': "The answer should be one of the options above, e.g. (A).",
  'ruin_names': "The answer should be one of the options above, e.g. (A).",
  'salient_translation_error_detection': "The answer should be one of the options above, e.g. (A).",
  'snarks': "The answer should be one of the options above, e.g. (A).",
  'sports_understanding': "The answer should be either yes or no, all lowercase.",
  'temporal_sequences': "The answer should be one of the options above, e.g. (A).",
  'tracking_shuffled_objects_five_objects': "The answer should be one of the options above. In other words, the answer has to be (A) or (B) or (C) or (D) or (E). The answer should not be anything else.",
  'tracking_shuffled_objects_seven_objects': "The answer should be one of the options above. In other words, the answer has to be (A) or (B) or (C) or (D) or (E) or (F) or (G). The answer should not be anything else.",
  'tracking_shuffled_objects_three_objects': "The answer should be one of the options above. In other words, the answer has to be (A) or (B) or (C). The answer should not be anything else.",
  'web_of_lies': "The answer should be either Yes or No, with the first letter capitalized.",
  'word_sorting': "The answer should be a list of words, separated by spaces, e.g. hello world.",
}

all_tasks = list(task_answer_format.keys())

num_tasks = len(all_tasks)
assert num_tasks == 28
few_shot_prompt = 3
num_examples_per_task = 3

import numpy as np
np.random.seed(0)

if PROTOCOL == "single_task":
  training_prompts = [build_prompt({task: list(range(few_shot_prompt))}) for task in all_tasks]
else:
  training_prompts = []
  for task in all_tasks:
    train_tasks_candidates = all_tasks.copy()
    train_tasks_candidates.remove(task)
    problematic_tasks = []
    if "logical_deduction" in task:
      problematic_tasks.extend(["logical_deduction_five_objects", "logical_deduction_seven_objects", "logical_deduction_three_objects"])
    if "tracking_shuffled_objects" in task:
      problematic_tasks.extend(["tracking_shuffled_objects_five_objects", "tracking_shuffled_objects_seven_objects", "tracking_shuffled_objects_three_objects"])
    # If CoC, these two tasks don't have proper prompts (because the trace are way too long.)
    if PROMPT_MODEL == "coc":
      problematic_tasks.extend(["dyck_languages", "geometric_shapes"])

    for pt in problematic_tasks:
      if pt in train_tasks_candidates:
        train_tasks_candidates.remove(pt)

    print(task, len(train_tasks_candidates))
    train_tasks = np.random.choice(train_tasks_candidates, 3, replace=False)
    training_prompts.append(build_prompt({task: [np.random.randint(0, num_examples_per_task)] for task in train_tasks}))

"""# Run experiments"""

results = dict()
bad_format = []
tasks_to_evaluate = None
# Comment out the line below to evaluate on all tasks
tasks_to_evaluate = ["hyperbaton"]
for training_prompt, test_task in zip(training_prompts, all_tasks):
    if tasks_to_evaluate is not None and test_task not in tasks_to_evaluate:
      continue

    result_file = f"{bbh_outputs}/{test_task}.json"
    if os.path.isfile(result_file):
      print(f"Result file {result_file} exists. Skipping {test_task}.")
      continue

    print(f"Task: {test_task}")
    with open(os.path.join(bbh_problems, f"{test_task}.json")) as f:
      problem = json.load(f)

    accuracies = []
    num_problems = len(problem["examples"])
    instructions = [
      "Task description: " + task_description[test_task] +
      "\n" + "Q: " + e["input"] +
      "\n" + "Answer format: " + task_answer_format[test_task] +
      ("\n" + "A:" if "zero-shot" not in MODEL else "")
      for e in problem["examples"]]
    targets = [e["target"] for e in problem["examples"]]
    for start_idx in trange(0, num_problems, BATCH_SIZE):
      end_idx = min(start_idx + BATCH_SIZE, num_problems)
      batch_instructions = instructions[start_idx : end_idx]
      success = False
      while True:
        try:
          batch_answer = evaluate(training_prompt, batch_instructions, verbose=True)
          success = True
          break
        except AssertionError as ae:
          raise ae
        except Exception as e:
          print("Encountering exception (likely OpenAI rate limit)", e)
          print("Sleep for 5 seconds")
          time.sleep(5)

      batch_targets = targets[start_idx : end_idx]
      print("#### Target ####")
      print(batch_targets)
      print("#### Answer ####")
      batch_answer = [str(a) for a in batch_answer]
      print(batch_answer)
      assert len(batch_answer) == len(batch_targets)

      batch_answer_fixed = []
      for answer, instruct in zip(batch_answer, batch_instructions):
        if answer == "":
          batch_answer_fixed.append(answer)
          continue
        answer = postprocess_answer(answer, test_task)
        batch_answer_fixed.append(answer)
      correct = [str(a) == t for a, t in zip(batch_answer_fixed, batch_targets)]
      print("#### Correct ####")
      print(correct)
      accuracies.extend(correct)
      print("Accuracy so far", np.mean(accuracies))

    print("Accuracy", np.mean(accuracies), len(accuracies))
    results[test_task] = np.mean(accuracies)

    with open(result_file, "w+") as f:
      json.dump({"accuracy": results[test_task]}, f)
    print("Written to result", result_file)

    with open(f"{bbh}/outputs/global_cache.json", "w+") as f:
      json.dump(remap_keys(GLOBAL_CACHE), f)

    with open(f"{bbh}/outputs/direct_answer_cache.json", "w+") as f:
      json.dump(remap_keys(DIRECT_ANSWER_CACHE), f)

    with open(f"{bbh}/outputs/cot_cache.json", "w+") as f:
      json.dump(remap_keys(COT_CACHE), f)

    with open(f"{bbh}/outputs/llm_trace_cache.json", "w+") as f:
      json.dump(remap_keys(LLM_TRACE_CACHE), f)

"""# Visualize results"""

TASKS_TO_VISUALIZE = "bbh" # @param ["bbh", "gsm"]
assert TASKS_TO_VISUALIZE in ["bbh", "gsm"]
combined_task_to_raw_tasks = {
  "logical_deduction": ["logical_deduction_three_objects", "logical_deduction_five_objects", "logical_deduction_seven_objects"],
  "tracking_shuffled_objects": ["tracking_shuffled_objects_three_objects", "tracking_shuffled_objects_five_objects", "tracking_shuffled_objects_seven_objects"],
}

# Uncomment below to visualize all tasks
combined_tasks_bbh = sorted(list({
  # 'boolean_expressions': "Evaluate the result of a random Boolean expression.",
  # 'causal_judgement': "Answer questions about causal attribution.",
  # 'date_understanding': "Infer the date from context.",
  # 'disambiguation_qa': "Clarify the meaning of sentences with ambiguous pronouns.",
  # 'dyck_languages': "Correctly close a Dyck-n word.",
  # 'formal_fallacies': "Distinguish deductively valid arguments from formal fallacies.",
  # 'geometric_shapes': "Name geometric shapes from their SVG paths.",
  'hyperbaton': "Order adjectives correctly in English sentences.",
  # 'logical_deduction_five_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  # 'logical_deduction_seven_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  # 'logical_deduction_three_objects': "A logical deduction task which requires deducing the order of a sequence of objects.",
  # 'movie_recommendation': "Recommend movies similar to the given list of movies.",
  # 'multistep_arithmetic_two': "Solve multi-step arithmetic problems.",
  # 'navigate': "Given a series of navigation instructions, determine whether one would end up back at the starting point.",
  # 'object_counting': "Questions that involve enumerating objects and asking the model to count them.",
  # 'penguins_in_a_table': "Answer questions about a table of penguins and their attributes.",
  # 'reasoning_about_colored_objects': "Answer extremely simple questions about the colors of objects on a surface.",
  # 'ruin_names': "Select the humorous edit that 'ruins' the input movie or musical artist name.",
  # 'salient_translation_error_detection': "Detect the type of error in an English translation of a German source sentence.",
  # 'snarks': "Determine which of two sentences is sarcastic.",
  # 'sports_understanding': "Determine whether an artificially constructed sentence relating to sports is plausible or not.",
  # 'temporal_sequences': "Answer questions about which times certain events could have occurred.",
  # 'tracking_shuffled_objects_five_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  # 'tracking_shuffled_objects_seven_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  # 'tracking_shuffled_objects_three_objects': "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.",
  # 'web_of_lies': "Evaluate a random boolean function expressed as a word problem.",
  # 'word_sorting': "Sort a list of words.",
}.keys()))
combined_tasks_gsm = sorted(list({
  'gsm': "Solve a grade school math problem.",  # GSM
}.keys()))

combined_tasks = combined_tasks_bbh if TASKS_TO_VISUALIZE == "bbh" else combined_tasks_gsm

if combined_tasks == combined_tasks_bbh:
  for combined_task, raw_tasks in combined_task_to_raw_tasks.items():
    has_raw_task = False
    for raw_task in raw_tasks:
      if raw_task in combined_task:
        has_raw_task = True
        combined_tasks.remove(raw_task)
    if has_raw_task:
      combined_tasks.append(combined_task)
combined_tasks.sort()

models = ["coc"]
# Uncomment below once results from other baselines have been produced.
# models = ["direct", "cot", "coc-python-only", "coc-llm-only", "coc-llm-only-trace", "coc-try-python-except-llm", "coc-try-python-except-llm-trace", "coc-try-python-except-direct", "coc-try-python-except-cot", "coc"]
# models = ["zero-shot-direct", "zero-shot-cot", "zero-shot-coc-llm-only", "zero-shot-coc-python-only"]
comparisons = models

results = {model: [] for model in comparisons}
for model in comparisons:
  for combined_task in combined_tasks:
    accuracies = []
    raw_tasks = combined_task_to_raw_tasks.get(combined_task, [combined_task])
    for task in raw_tasks:
      result_file = f"{bbh}/outputs/{PROTOCOL}/{ENGINE}/{model}/{task}.json"
      with open(result_file) as f:
        acc = json.load(f)["accuracy"]
        accuracies.append(acc)
    results[model].append(np.mean(accuracies))

xs = combined_tasks + ["average"]
ys = {engine: results[engine] + [np.mean(results[engine])] for engine in comparisons}

dict_form = {engine: {x: y for x, y in zip(xs, ys[engine])} for engine in comparisons}

x = np.arange(len(xs))  # the label locations
width = 1.0 / (len(comparisons) + 1)  # the width of the bars
multiplier = 0

fig, ax = plt.subplots(layout='constrained')

for model, accuracy in ys.items():
    offset = width * multiplier
    rects = ax.bar(x + offset, accuracy, width, label=model)
    ax.bar_label(rects, padding=3, fmt='{:,.2f}')
    multiplier += 1

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Accuracy')
ax.set_xticks(x + width, xs, fontsize=6)
ax.legend(loc='upper left')

plt.show()

for key, acc in ys.items():
  print(key, "{:.2f}".format(acc[-1]))